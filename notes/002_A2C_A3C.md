# Asynchronous Methods for Deep Reinforcement Learning

## Overview

The authors proposed an asynchronous optimization method applicable to different model-free RL methods (value-based and policy-based), different update methods (on-policy and off-policy), and different domains (discrete and continuous).

They apply their method to four standard reinforcement learning algorithms

- SARSA
- One-step DQN
- N-step DQN
- Advantage Actor-Critic (A2C - a particular case of the [REINFORCE (or vanilla policy gradient)](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html) algorithm, where the advantage is used)
    
The idea is to keep global parameters of any parametrized function used during training, and asynchronously execute multiple agents in parallel. Each agent will have its own copy of the environment. Then, **the update steps are done asynchronously in an on-policy fashion** [[illustration]](./assets/asynchronous.jpg).
**NOTE:** Although the asynchronous method is the main contribution of this paper, I'm going to highlight only the asynchronous method in A2C because it's more important in my context.

### Asynchronous Advantage Actor Critic (A3C)

They refer to Advantage Actor-Critic (A2C) algorithm as the specific case of REINFORCE algorithm, where the baseline $b_t(s_t)$ is a learned estimate of the value function, as follows
$$b_t(s_t) \approx V^{\pi}(s_t)$$

$$\nabla_{\theta}\text{log}_{\pi}(a_t|s_t; \theta)(R_t - b_t(s_t))$$

In this context, the quantity $R_t - b_t$ used to scale the policy gradient can be seen as an estimate of the advantage $A(a_t,s_t)$
$$A(a_t,s_t) = Q(a_t,s_t) - V(s_t)$$

- $R_t$ is an estimate of $Q^{\pi}(a_t, s_t)$
- $b_t$ is an estimate of $V^{\pi}(s_t)$

This method is called an actor-critic [[illustration]](./assets/actor-critic.jpg) because
- the policy $\pi$ can be viewed as the **actor**
- and the baseline $b_t$ can be viewed as the **critic**

Consequently, they introduce the asynchronous method to perform the on-policy learning of A2C stably. In such a way, they called it **Asynchronous Advantage Actor Critic** (A3C). You can find the pseudocode here: [A3C pseudocode](./assets/A3C_pseudocode.png).

## Key Features
- They address the problem of non-stationary and correlated RL updates by asynchronously updating some global parameters by multiple agents, which have their own copy of the environment. As each agent has its own interaction, they guarantee more diversity.
- The parallel actors maximize diversity because they can explore completely different scenarios of the environment. In such a way, the updates are less correlated and consequently they don't use replay memory for DQN.
- Additionally, the parallel actor enables to substitute of batch learning (common in DQN) by an on-policy learning.


## Implications

- They can train the reinforcement learning algorithm in less time using only CPU.
- By using parallel actors, training on-policy RL methods such as Sarsa, and actor-critic becomes more stable.
- They don't need to use experience replay in DQN. However, the incorporation of the replay buffer could improve the data efficiency of these by reusing old data.

## Open Questions (for me)

- In the n-step DQN and A2C, a **forward view** of the rollout (trajectory) was used to calculate the estimation of $R_t$. However, there exist other **backward view** (eligibility traces), which is more common. How it works?
- SARSA is an on-policy or off-policy method and why? I think on-policy
- What is the relation between target and behavior parameters when talking about on-policy and off-policy?
- How do they calculate the entropy? and why is it useful for tasks requiring hierarchical behavior? Review other methods ([TRPO](https://spinningup.openai.com/en/latest/algorithms/trpo.html), or [PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html))

## Conclusions

The proposed method enables a wide range of applications in four different model-free methods, and discrete and continuous domains. It is able to reduce the training time by only using CPU with threads. The parallel actors encourage more diversity (without the use of Experience Replay in DQN), and consequently less correlation and more stationarity.
